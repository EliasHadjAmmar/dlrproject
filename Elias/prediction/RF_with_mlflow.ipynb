{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.chdir(\"/Users/eliashadjammar/GitHub/dlrproject\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a handy function for later\n",
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    return rmse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"drive/aggregates/alldata_with_prices.csv\")\n",
    "raw_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the data usable\n",
    "In its current state it's not possible to train a model on `raw_data`. There are two big issues: 1) there are more columns (644) than rows (421), and 2) there are a lot of missing values. In particular, a majority of the Building_Type columns is probably useless. On the other hand, some may not be.\n",
    "\n",
    "I could replace all the NAs in Building_Type columns with zeroes. I'm not sure about it, but it's an option to keep in mind.\n",
    "Another option would be to get rid of most of these columns, only keeping the ones with the most information content.\n",
    "\n",
    "How about this: drop those columns which are missing for more than 20% of neighborhoods.\n",
    "Then after that, drop rows with missing values and see how much is left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([raw_data[col] for col in raw_data.columns if raw_data[col].isna().sum() < 80]).transpose()\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, after getting rid of literally all BuildingType data except for chapels, hospitals, museums, and stables, we don't need to drop anything else anymore. And the segregation measures are also still there.\n",
    "\n",
    "We still want to get rid of the id and neighborhood columns, as we don't need them here. We do want to keep the city_id, though, since including that is basically like region fixed effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = data.drop([\"id\", \"Neighborhood_FID\"], axis=1)\n",
    "clean_data['city_id'] = clean_data['city_id'].astype('category')\n",
    "citydummies = pd.get_dummies(clean_data['city_id'], prefix=\"city\", prefix_sep=\"\")\n",
    "clean_data = pd.concat([citydummies, clean_data],axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets. (0.75, 0.25) split.\n",
    "train, test = train_test_split(clean_data)\n",
    "\n",
    "# The predicted column is \"Land_Value\" which is a scalar\n",
    "train_x = train.drop([\"Land_Value\"], axis=1)\n",
    "test_x = test.drop([\"Land_Value\"], axis=1)\n",
    "train_y = train[[\"Land_Value\"]]\n",
    "test_y = test[[\"Land_Value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "test_x = scaler.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "n_estimators = 100\n",
    "max_depth = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9v/qhkb598n1mv3kmfvr16ckv3m0000gn/T/ipykernel_98249/3381449617.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(train_x, train_y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (n_estimators=100.000000, max_depth=40.000000):\n",
      "  RMSE: 900.5850977648228\n",
      "  MAE: 387.17586792452846\n",
      "  R2: 0.6582924570696094\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    # Test the model\n",
    "    predicted_qualities = model.predict(test_x)\n",
    "\n",
    "    # Compute metrics\n",
    "    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "    print(\"Random Forest (n_estimators={:f}, max_depth={:f}):\".format(n_estimators, max_depth))\n",
    "    print(\"  RMSE: %s\" % rmse)\n",
    "    print(\"  MAE: %s\" % mae)\n",
    "    print(\"  R2: %s\" % r2)\n",
    "\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "\n",
    "    tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "    # Model registry does not work with file store\n",
    "    if tracking_url_type_store != \"file\":\n",
    "        # Register the model\n",
    "        # There are other ways to use the Model Registry, which depends on the use case,\n",
    "        # please refer to the doc for more information:\n",
    "        # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "        mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"RandomForest\")\n",
    "    else:\n",
    "        mlflow.sklearn.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ifo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
